{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ5PRVCTQsLe",
    "tags": []
   },
   "source": [
    "# **Regression using bagging on decision trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4wSeAQaPz0E"
   },
   "source": [
    "## **library import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f06YoyjJPkMB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHi3oCuDP5A0"
   },
   "source": [
    "## **class Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "901mUOkhP4ax"
   },
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, split_feature_index = None, split_treshhold = None, left_subtree = None,\n",
    "                 right_subtree = None, node_variance_reduction = None, leaf_node_value = None):\n",
    "        '''This class is simply a collection of data that the top of the decision tree should store.'''\n",
    "\n",
    "        # Для внутренних вершин, содержащих предикат\n",
    "        self.split_feature_index = split_feature_index\n",
    "        self.split_treshhold = split_treshhold\n",
    "        self.left_subtree = left_subtree\n",
    "        self.right_subtree = right_subtree\n",
    "        self.node_variance_reduction = node_variance_reduction\n",
    "\n",
    "        # Для листовых вершин, содержащих лишь значение\n",
    "        self.leaf_node_value = leaf_node_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qZ6tHzmP4rq"
   },
   "source": [
    "## **class Decision_tree_classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi9SrOXU6YGs"
   },
   "outputs": [],
   "source": [
    "class Decision_tree_regression():\n",
    "    def __init__(self, node_min_samples_quant = 2, max_depth = 2):\n",
    "        '''This is the model class for decision tree regression'''\n",
    "\n",
    "        self._node_min_samples_quant = node_min_samples_quant\n",
    "        self._max_depth = max_depth\n",
    "\n",
    "        self._root = None\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        '''This function we need for training the model on the training set'''\n",
    "\n",
    "        dataset = np.concatenate((X, Y), axis = 1, dtype = object)\n",
    "        self._root = self._build_tree(dataset)\n",
    "    \n",
    "    def _build_tree(self, dataset, current_depth = 0):\n",
    "        '''This function we need for building а deсision tree'''\n",
    "        X = dataset[:, :-1]\n",
    "        Y = dataset[:, -1]\n",
    "        \n",
    "        dataset_rows_quant, dataset_features_quant = X.shape\n",
    "\n",
    "        if dataset_rows_quant >= self._node_min_samples_quant and current_depth <= self._max_depth:\n",
    "            best_split_values = self._get_best_split(dataset, dataset_features_quant)\n",
    "\n",
    "            if best_split_values[\"node_variance_reduction\"] > 0:\n",
    "                left_subtree = self._build_tree(best_split_values[\"dataset_left_split\"], current_depth + 1)\n",
    "                right_subtree = self._build_tree(best_split_values[\"dataset_right_split\"], current_depth + 1)\n",
    "\n",
    "                return Node(best_split_values[\"split_feature_index\"], best_split_values[\"split_treshhold\"],\n",
    "                            left_subtree, right_subtree, best_split_values[\"node_variance_reduction\"])\n",
    "        \n",
    "        leaf_node_value = self._calculate_leaf_node_value(Y)\n",
    "\n",
    "        return Node(leaf_node_value = leaf_node_value)\n",
    "\n",
    "    def _get_best_split(self, dataset, dataset_features_quant):\n",
    "        '''This function finds and makes the best split'''\n",
    "\n",
    "        max_variance_reduction = -float(\"inf\")\n",
    "        best_split_values = {}\n",
    "\n",
    "        for current_split_feature_idx in range(dataset_features_quant):\n",
    "            possible_treshholds = np.unique(dataset[:, current_split_feature_idx])\n",
    "\n",
    "            for current_split_treshhold in possible_treshholds:\n",
    "                dataset_left_split, dataset_right_split = self._make_split(dataset, current_split_feature_idx,\n",
    "                                                                           current_split_treshhold)\n",
    "\n",
    "                if len(dataset_left_split) > 0 and len(dataset_right_split) > 0:\n",
    "                    current_variance_reduction = self._calculate_variance_reduction(dataset, dataset_left_split,\n",
    "                                                                                dataset_right_split)\n",
    "\n",
    "                    if current_variance_reduction > max_variance_reduction:\n",
    "                        best_split_values[\"split_feature_index\"] = current_split_feature_idx\n",
    "                        best_split_values[\"split_treshhold\"] = current_split_treshhold\n",
    "                        best_split_values[\"dataset_left_split\"] = dataset_left_split\n",
    "                        best_split_values[\"dataset_right_split\"] = dataset_right_split\n",
    "                        best_split_values[\"node_variance_reduction\"] = current_variance_reduction\n",
    "                        max_variance_reduction = current_variance_reduction\n",
    "        \n",
    "        return best_split_values\n",
    "\n",
    "    def _make_split(self, dataset, dataset_split_feature_index, dataset_split_treshhold):\n",
    "        '''This function produces the best sample split'''\n",
    "        dataset_left_split = np.array([row for row in dataset if row[dataset_split_feature_index] <= dataset_split_treshhold])\n",
    "        dataset_right_split = np.array([row for row in dataset if row[dataset_split_feature_index] > dataset_split_treshhold])\n",
    "\n",
    "        return dataset_left_split, dataset_right_split\n",
    "    \n",
    "    def _calculate_variance_reduction(self, dataset, dataset_left_split, dataset_right_split):\n",
    "        '''This function calculates the reduction between the variance of the parent\n",
    "        node and the sum of the variances of the child nodes'''\n",
    "\n",
    "        dataset_targets = dataset[:, -1]\n",
    "        dataset_left_split_targets = dataset_left_split[:, -1]\n",
    "        dataset_right_split_targets = dataset_right_split[:, -1]\n",
    "\n",
    "        card_dataset_left_split = len(dataset_left_split)\n",
    "        card_dataset_right_split = len(dataset_right_split)\n",
    "        card_dataset = len(dataset_targets)\n",
    "\n",
    "        dataset_left_split_targets_variance = np.var(dataset_left_split_targets)\n",
    "        dataset_right_split_targets_variance = np.var(dataset_right_split_targets)\n",
    "        dataset_targets_variance = np.var(dataset_targets)\n",
    "\n",
    "        dataset_variance = card_dataset * dataset_targets_variance\n",
    "        split_variance = card_dataset_left_split * dataset_left_split_targets_variance + \\\n",
    "                         card_dataset_right_split * dataset_right_split_targets_variance\n",
    "\n",
    "        variance_reduction = dataset_variance - split_variance\n",
    "\n",
    "        return variance_reduction\n",
    "\n",
    "    def _calculate_leaf_node_value(self, dataset_targets):\n",
    "        '''This function calculates the value that will be contained in the node if it is a leaf'''\n",
    "\n",
    "        return np.mean(dataset_targets)\n",
    "\n",
    "    def predict(self, data):\n",
    "        '''This function predicts the values (class labels) for the sample that was passed as an argument'''\n",
    "\n",
    "        predictions = np.array([self._make_prediction(data_sample, self._root) for data_sample in data])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def _make_prediction(self, sample, current_node):\n",
    "        '''This function predicts a value (class label) for a particular sample object'''\n",
    "\n",
    "        if current_node.leaf_node_value != None:\n",
    "            return current_node.leaf_node_value\n",
    "        else:\n",
    "            feature_value = sample[current_node.split_feature_index]\n",
    "\n",
    "            if feature_value <= current_node.split_treshhold:\n",
    "                return self._make_prediction(sample, current_node.left_subtree)\n",
    "            else:\n",
    "                return self._make_prediction(sample, current_node.right_subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBnMoePw4Ct7"
   },
   "source": [
    "## **class Bagging_using_decision_trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkDiTQus4IQp"
   },
   "outputs": [],
   "source": [
    "class Bagging_using_decision_trees():\n",
    "    def __init__(self, dt_node_min_samples_quant = 2, dt_max_depth = 2, dt_quant = 10, bootstrap_set_size = 10):\n",
    "        ''''''\n",
    "\n",
    "        self._dt_node_min_samples_quant = dt_node_min_samples_quant\n",
    "        self._dt_max_depth = dt_max_depth\n",
    "        self._dt_quant = dt_quant\n",
    "        self._bootstrap_set_size = bootstrap_set_size\n",
    "        self._dataset_bootstrap_sets = []\n",
    "        self._decision_trees_base_algorithms = []\n",
    "\n",
    "    def fit(X, Y):\n",
    "        '''This function we need for training the model on the training set'''\n",
    "        dataset = np.concatenate((X, Y), axis = 1, dtype = object)\n",
    "\n",
    "        self._dataset_bootstrap_sets = make_set_using_bootstrap(dataset, self._dt_quant,\n",
    "                                                                self._bootstrap_set_size)\n",
    "\n",
    "        for dataset_bootstrap_set in self._dataset_bootstrap_sets:\n",
    "            X = dataset_bootstrap_set[:, :-1]\n",
    "            Y = dataset_bootstrap_set[:, -1]\n",
    "\n",
    "            model = Decision_tree_regression(node_min_samples_quant = self._dt_node_min_samples_quant,\n",
    "                                             max_depth = self._dt_max_depth)\n",
    "            model.fit(X, Y)\n",
    "\n",
    "            self._decision_trees_base_algorithms.append(model)\n",
    "    \n",
    "    def _bootstrap():\n",
    "        pass\n",
    "\n",
    "    def _predict():\n",
    "        '''This function predicts the values for the sample that was passed as an argument'''\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FK-N5bM64JQp"
   },
   "source": [
    "## **initialize input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "209Gd39r4Of7",
    "outputId": "d813078b-7bc6-40a5-f154-0df5f5be3ed6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD://ADMP_Anastasia/machine_learning/datasets/dataset_for_bagging_using_decision_trees.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"D://ADMP_Anastasia/machine_learning/datasets/dataset_for_bagging_using_decision_trees.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HmQI1Dl_Tlv"
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "Y = df.iloc[:, -1].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl6R94FtBuCe"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 1/3, random_state = 42)\n",
    "\n",
    "X_test_len = len(X_test)\n",
    "Y_test_len = len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_-hGW_QBHKu"
   },
   "source": [
    "## **creating and training decision tree classifier model on our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nvzjHkkBG9G"
   },
   "outputs": [],
   "source": [
    "model = Bagging_using_decision_trees(dt_node_min_samples_quant = 2, dt_max_depth = 7,\n",
    "                                     dt_quant = 10, bootstrap_set_size = 20)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0jyTTwuFM6_"
   },
   "source": [
    "## **making predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPu9csXLFMou"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = Y_pred.reshape(-1, 1)\n",
    "Y_pred_len = len(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gG6pqAlErnH"
   },
   "source": [
    "## **displaying a graph with a test sample, true targets and model predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1v5KOalZ2zK"
   },
   "outputs": [],
   "source": [
    "graph_X_values = X_test[:, 0]\n",
    "graph_Y_test_values = Y_test\n",
    "grapg_Y_pred_values = Y_pred\n",
    "\n",
    "figure, ax = plt.subplots()\n",
    "\n",
    "figure.set_figwidth(7)\n",
    "figure.set_figheight(5)\n",
    "\n",
    "ax.set_title('Тестовая выборка и предсказания модели')\n",
    "\n",
    "ax.scatter(graph_X_values, graph_Y_test_values, color = \"tomato\",\n",
    "           label = \"Тестовая выборка\", marker = \"x\")\n",
    "ax.scatter(graph_X_values, grapg_Y_pred_values, color = \"gold\",\n",
    "           edgecolors='orange', label = \"Предсказания модели\", alpha = 0.5)\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9AdVVR5soXB"
   },
   "source": [
    "## **displaying accuracy score of our predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orcAneXTs1dK"
   },
   "outputs": [],
   "source": [
    "mean_squared_error_value = mean_squared_error(Y_test, Y_pred)\n",
    "mean_squared_error_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWFUurp7T8jS"
   },
   "source": [
    "## **displaying a table with test data and targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxlYReEYUIXC"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(np.concatenate((X_test, Y_test), axis = 1),\n",
    "                       columns = [\"x\", \"x**3\", \"x**5\", \"x**7\", \"y\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn6WvGJyUDD-"
   },
   "source": [
    "## **displaying a table with test data and our model predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9T9NrvoXyQV"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mconcatenate((X_test, Y_pred), axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      2\u001b[0m                        columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx**3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx**5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx**7\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      3\u001b[0m pred_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(np.concatenate((X_test, Y_pred), axis = 1),\n",
    "                       columns = [\"x\", \"x**3\", \"x**5\", \"x**7\", \"y\"])\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
